# BananaDoc Backend API

Python Flask API for banana leaf deficiency detection using TensorFlow/Keras ML models.

## ğŸ¯ Overview

This backend provides:
- REST API for image analysis
- TensorFlow model serving
- Gemini AI chat integration
- Model training pipeline
- Docker deployment support

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or later
- pip
- Virtual environment (recommended)
- (Optional) Docker & Docker Compose

### Installation

1. **Navigate to backend directory**
   ```bash
   cd backend
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   # Create .env file with your configuration
   echo "GEMINI_API_KEY=your_key_here" > .env
   echo "PORT=5002" >> .env
   ```

5. **Run the API server**
   ```bash
   python run_api.py
   ```

The API will start on `http://localhost:5002`

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file in the backend directory:

```bash
# Gemini API Configuration
GEMINI_API_KEY=your_gemini_api_key_here

# Server Configuration
PORT=5002
HOST=0.0.0.0
DEBUG=True

# Model Configuration
MODEL_PATH=models_runtime/banana_mobile_model.tflite
CLASS_MAPPING_PATH=models_runtime/mobile_class_mapping.txt
```

## ğŸ—ï¸ Project Structure

```
backend/
â”œâ”€â”€ api/                          # API endpoints
â”‚   â”œâ”€â”€ banana_deficiency_api.py  # Main prediction API
â”‚   â””â”€â”€ chat_server.py            # Chat/conversation API
â”œâ”€â”€ utils/                        # Utility modules
â”‚   â”œâ”€â”€ deficiency_info.py        # Deficiency information
â”‚   â”œâ”€â”€ gemini_handler.py         # Gemini API integration
â”‚   â”œâ”€â”€ image_preprocessor.py     # Image preprocessing
â”‚   â””â”€â”€ model_loader.py           # Model loading utilities
â”œâ”€â”€ models_runtime/               # Production ML models
â”‚   â”œâ”€â”€ banana_mobile_model.tflite
â”‚   â”œâ”€â”€ mobile_class_mapping.txt
â”‚   â””â”€â”€ model_metadata.json
â”œâ”€â”€ training/                     # Model training scripts
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ create_mobile_model.py
â”‚   â”œâ”€â”€ finetune_mobile_model.py
â”‚   â””â”€â”€ convert_to_tflite.py
â”œâ”€â”€ data/                         # Runtime data
â”‚   â””â”€â”€ conversation_context.json
â”œâ”€â”€ docs/                         # Documentation
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ run_api.py                    # API entry point
â”œâ”€â”€ Dockerfile                    # Docker configuration
â””â”€â”€ docker-compose.yml            # Docker Compose setup
```

## ğŸ“¡ API Endpoints

### Health Check
```
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2026-02-07T10:00:00Z"
}
```

### Predict Deficiency
```
POST /predict
Content-Type: multipart/form-data
```

**Request:**
- `image`: Image file (JPEG/PNG)

**Response:**
```json
{
  "prediction": "Nitrogen Deficiency",
  "confidence": 0.92,
  "recommendations": [...],
  "timestamp": "2026-02-07T10:00:00Z"
}
```

### Chat / AI Analysis
```
POST /chat
Content-Type: application/json
```

## ğŸ³ Docker Deployment

### Using Docker Compose (Recommended)

```bash
# Build and run
docker-compose up --build

# Run in detached mode
docker-compose up -d

# Stop
docker-compose down
```

## ğŸ¤– Model Training

### Training a New Model

1. Prepare your dataset in `training/data/`
2. Run training script:
   ```bash
   cd training
   python train_model.py
   ```
3. Create mobile-optimized model:
   ```bash
   python create_mobile_model.py
   ```

See [training/README.md](training/README.md) for detailed instructions.

## ğŸ§ª Testing

```bash
# Health check
curl http://localhost:5002/health

# Predict with image
curl -X POST http://localhost:5002/predict \
  -F "image=@path/to/image.jpg"
```

## ğŸ“¦ Dependencies

Key packages:
- `flask` - Web framework
- `tensorflow` - ML framework
- `pillow` - Image processing
- `google-generativeai` - Gemini API

See [requirements.txt](requirements.txt) for complete list.

## ğŸ” Troubleshooting

**Issue: Port already in use**
```bash
# Change port in .env or kill process
lsof -ti:5002 | xargs kill -9
```

**Issue: Model not found**
```bash
# Verify model files exist in models_runtime/
ls -la models_runtime/
```

## ğŸ“„ License

[Your License Here]

---

**Framework:** Flask  
**Language:** Python  
**ML Framework:** TensorFlow/Keras  
**AI Integration:** Google Gemini
